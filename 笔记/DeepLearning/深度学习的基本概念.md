# 深度学习的基本概念

---------------------------------------

## 神经网络继承

---------------------------------------------------

### 神经元（Neuron）

神经元是构成神经网络的基本结构。在神经网络中，神经元接收输入，处理它并产生输出，输出又被发送到其他神经元做进一步处理，或者作为最终结果进行输入

### 权重（Weights）

当输入进入神经元时，会给他乘以一个权重。一个神经元有多个输入，对多个输入分配权重。训练后的神经元会对重要的权重赋较高的权重，对微不足道的权重赋0权重。

###  偏差（Bias）

偏差是权重外另一个线性分量。它被加到权重与输入相乘的结果中去。添加的偏差是为了让结果更接近真实值，这是输入变换的最终线性分量。

### 激活函数（Activation Function）

将线性分量应用于输入后，需要一个非线性函数，将神经元的特征保留并进行应收。激活函数经输入信号转换为输出信号。应用激活函数后的输出看起来像$f(a\times W_1+b)$,其中，$f()$就是激活函数。

将n个输入给定$X_1$到$X_n$,而与其相对应的权重为$Wk_1$到$Wk_n$。有一个给定值为b的偏差。权重首先乘以与其对应的输入，然后与偏差加到一起。产生的值为$u$。
$$
U=\sum W\times X+b
$$
激活函数被应用于$u$,即$f(u)$，并且从神经元接收到的最终输出为$y=f(u)$。

最常用的激活函数就是sigmoid、ReLU和softmax。

- sigmoid----最常用的激活函数之一，被定义为：$sigmoid(x)=1/(1+e^x)$。

sigmoid变换产生一个值为0到1之间的平滑范围值。可以观察到输入值略有变化时输出值发生的变化。

- ReLU(整流线性单元)----最近的网络更喜欢使用ReLU激活函数老处理隐藏层。该函数定义为：$f(x)=max(x,0)$。当$x>0$时，函数的输出值为$x$;当$x \leq0$时，输出值为0.

使用ReLU函数最主要的好处是，对于大于0的所有输入来说，它都有一个不变的导数值。常数导数值有助于网络训练进行得更快。

- softmax----softmax激活函数通常用于多分类问题的输出层。它与sigmoid函数类似，唯一的区别就是输出被归一化为总和为1.在一个多分类问题中，softmax函数为每个类分配值，这个值可以理解为概率。

### 神经网络（Neural Network）

神经网络由相互连接的神经元形成，构成了深度学习的核心，它的目标是找到一个未知函数的近似值。这些神经元具有权重，并在网络训练期间根据错误来进行更新。激活函数为线性模型增加非线性因素，并基于线性组合生成输出。

### 输入/输出/隐藏层（Input/Output/Hidden Layer）

- 输入层是接收输入那一层，本质上是网络的第一层

- 输出层是生成输出的那一层，也可以说是网络的最终层

- 处理层是网络中的隐藏层，这些隐藏层对传入数据执行特定处理，并将生成的输出传递到下一层。

输入层和输出层是可见的，而中间层是隐藏的

### 多层感知器（MLP）

单个神经元无法执行高度复杂的任务。因此，使用堆叠的神经元生成需要的输出。每个层中的所有神经元都连接到下一层的所有神经元。因此网络也被称为完全连接网络，又叫多层感知器。

各层的神经元数量的选择是一个试错的过程。通常情况下，输入层的神经元数量与输入数据的维度相同。输出层神经元的数量，在回归问题和二元分类中通常为一个神经元，在多分类问题中通常与类别数相同。隐藏层的神经元数量可以自由设定，通过试错找到一个最合适的值，这通常是由通过网络的信息量决定的。在通常情况下，分类问题隐藏层的神经元素数量可以设定为类别数量的5~10倍，回归问题隐藏层的神经元数量可以设定为输入数据维度的2-3倍.

### 正向传播（Forward Propagation）

### 成本函数（Cost Function）

### 梯度下降（Gradient Descent）

### 学习率（Learning Rate）

### 反向传播（Back Propagation）

### 批次（Batches）

### 周期（Epochs）

### 丢弃（Dropout）



## 卷积神经网络

卷积神经网络主要应用于计算机视觉。假设有一个输入大小为（28，28，3）的图像，如果使用正常的圣经网络，将有2352（28$\times$28$\times$3）个参数。并且随着图像大小的增加参数的数量将会变得非常大，“卷积”图像能够有效减少参数的数量。

### 滤波器（Filters）

### 池化层（Pooling）

### 填充（Padding）

### 数据增强（Data Augmentation）

## 循环神经网络

循环神经网络特别适用于序列数据，先前的输出用于预测下一个输出。隐藏神经元内的循环使他们能够储存有关前一个单元的信息一段时间，以便能够预测输出，并且隐藏层的输出在t时间戳内再次反馈到隐藏层。

只有在完成所有的时间戳后，循环神经元的输出才能进入下一层。发送的输出更广泛，状态信息保留的时间也更长，并根据展开的网络将错误反向传播且更新权重，这被称为通过时间的反向传播（BPTT）

### 循环神经元（Recurrent Neuron）

### 梯度消失问题（Vanishing Gradient Problem）

### 梯度爆炸问题（Exploding Gradient Problem）





